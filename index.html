<!DOCTYPE html>
<html>
<head>
  <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-MCJXW9YP0V"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-MCJXW9YP0V');
</script>
  <meta charset="utf-8">
  <meta name="description"
        content="Enhancing Embodied Object Detection through Language-Image Pre-training and Implicit Object Memory.">
  <meta name="keywords" content="Object Detection, Language-Image Pre-training, Implicit Object Memory">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Enhancing Embodied Object Detection through Language-Image Pre-training and Implicit Object Memory.</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>

<!-- Remove navbar -->
<!--   <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div> -->

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Enhancing Embodied Object Detection through Language-Image Pre-training and Implicit Object Memory</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=kBdGmzAAAAAJ&hl=en">Nicolas Harvey Chapman</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://ferasdayoub.com/">Feras Dayoub</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://www.qut.edu.au/about/our-people/academic-profiles/will.browne">Will Browne</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.qut.edu.au/about/our-people/academic-profiles/c.lehnert">Chris Lehnert</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Queensland University of Technology,</span>
            <span class="author-block"><sup>2</sup>University of Adelaide</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2402.03721"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2402.03721"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/nhcha6"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
<!--               <!-- Video Link. -->
<!--               <span class="link-block"> -->
<!--                 <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA" -->
<!--                    class="external-link button is-normal is-rounded is-dark"> -->
<!--                   <span class="icon"> -->
<!--                       <i class="fab fa-youtube"></i> -->
<!--                   </span> -->
<!--                   <span>Video</span> -->
<!--                 </a> -->
<!--               </span> -->
<!--               <!-- Dataset Link. -->
<!--               <span class="link-block"> -->
<!--                 <a href="https://github.com/google/nerfies/releases/tag/0.1" -->
<!--                    class="external-link button is-normal is-rounded is-dark"> -->
<!--                   <span class="icon"> -->
<!--                       <i class="far fa-images"></i> -->
<!--                   </span> -->
<!--                   <span>Data</span> -->
<!--                   </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>
  
<section class="hero teaser">
  <div class="container is-max-desktop">
<!--     <h2 class="title is-3">Video</h2> -->
    <div class="hero-body">
      <video id="teaser" autoplay controls muted loop playsinline height="100%">
        <source src="./media/entire_video.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        We study the task of <b>embodied object detection</b>, where a robot must detect objects in a multi-modal data stream collected from indoor scenes.
          Our solution enhances embodied object detection performance using <b>language-image pre-training</b> and a <b>novel external memory</b>.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          
          <p>
            Deep-learning and large scale language-image training have produced image object detectors that generalise well to diverse environments and semantic classes. 
            However, single-image object detectors trained on internet data are not optimally tailored for the embodied conditions inherent in robotics. 
            Instead, robots must detect objects from complex multi-modal data streams involving depth, localisation and temporal correlation, a task termed embodied object detection.
          </p>
          <p>
            Paradigms such as Video Object Detection (VOD) and Semantic Mapping have been proposed to leverage such embodied data streams, but existing work fails to enhance performance using language-image training. 
            In response, we investigate how an image object detector pre-trained using language-image data can be extended to perform embodied object detection.
            We propose a novel implicit object memory that uses projective geometry to aggregate the features of detected objects across long temporal horizons. 
            The spatial and temporal information accumulated in memory is then used to enhance the image features of the base detector. 
          </p>
          <p>
           When tested on embodied data streams sampled from diverse indoor scenes, our approach improves the base object detector by 3.09 mAP, outperforming alternative external memories designed for VOD and Semantic Mapping. 
           Our method also shows a significant improvement of 16.90 mAP relative to baselines that perform embodied object detection without first training on language-image data, and is robust to sensor noise and domain shift experienced in real-world deployment.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>

<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video id="depth" autoplay muted loop playsinline height="150%">
            <source src="./media/depth.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video id="rgb" autoplay muted loop playsinline height="150%">
            <source src="./media/rgb.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video id="localisation" autoplay muted loop playsinline height="150%">
            <source src="./media/LOCALISATION.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->

<section class="section">
  <div class="container is-max-desktop">
    <div class="content">
          <h2 class="title is-3">Embodied Object Detection</h2>
          <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p>
    </div>
    <div class="columns is-centered">

      <!-- Depth. -->
      <div class="column">
        <div class="content">
          <video id="depth" autoplay muted loop playsinline height="100%">
            <source src="./media/depth (2).mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ Depth. -->

      <!-- RGB. -->
      <div class="column">
        <div class="content">
          <video id="rgb" autoplay muted loop playsinline height="100%">
            <source src="./media/rgb (1).mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ RGB -->

      <!-- Localisation -->
      <div class="column">
        <div class="content">
            <video id="localisation" autoplay muted loop playsinline height="100%">
              <source src="./media/LOCALISATION (2) (1).mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>
    <!--/ Localisation -->

<section class="section">
  <div class="container is-max-desktop">
    <div class="content">
      <h2 class="title is-3">Method</h2>
      <p>
        Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
        would be impossible without nerfies since it would require going through a wall.
      </p>
    </div>
    <div class="content">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./media/method (1).mp4"
                type="video/mp4">
      </video>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{chapman2024enhancing,
  title={Enhancing Embodied Object Detection through Language-Image Pre-training and Implicit Object Memory},
  author={Chapman, Nicolas Harvey and Dayoub, Feras and Browne, Will and Lehnert, Chris},
  journal={arXiv preprint arXiv:2402.03721},
  year={2024}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
        <div class="content has-text-centered">
          Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">here</a>
          </p>
        </div>
    </div>
  </div>
</footer>

</body>
</html>
